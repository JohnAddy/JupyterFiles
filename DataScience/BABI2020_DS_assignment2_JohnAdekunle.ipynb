{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.8, subjectivity=0.75)\n",
      "Sentiment(polarity=0.3352272727272727, subjectivity=0.5272727272727272)\n",
      "Sentiment(polarity=0.3958333333333333, subjectivity=0.4666666666666666)\n"
     ]
    }
   ],
   "source": [
    "#Sentiment analysis Assignment 2\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"great\")\n",
    "print(blob.sentiment)\n",
    "blob = TextBlob(\"I would love to meet your new friend!\")\n",
    "print(blob.sentiment)\n",
    "blob = TextBlob(\"I am so excited to move on to the next module, it looks very interesting!\")\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. N-grams are basically all the combinations of adjacent words or letters of the length n that can be found in your text.\n",
    "\n",
    "2. The reason to use n-grams is that they capture the language structure from a statistical point of view, like what letter or    word is likely to follow the given one\n",
    "\n",
    "3. There would be a risk of overfitting data because as n increases the number of n increases the number of n-grams decrease so    it will contain more information about a particular word for example but it will be less easy to generalize to other data      sets.\n",
    "\n",
    "4. The max_features parameter is used so that you can specify the amount of features that the CountVectorizer will choose to      keep in its vocabulary.\n",
    "\n",
    "5. We might want to use them because they help determine the sentiment of a text, for example an exclamation mark holds meaning    in sentiment.\n",
    "\n",
    "6. Features are words, phrases or aspects that you have specified to be searched for in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\johna\\anaconda3\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: six in c:\\users\\johna\\anaconda3\\lib\\site-packages (from langdetect) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sv\n",
      "ru\n",
      "es\n",
      "zh-cn\n",
      "hi\n",
      "ja\n"
     ]
    }
   ],
   "source": [
    "# Specifying the language for \n",
    "# detection \n",
    "\n",
    "\n",
    "print(detect(\"Hamk Student are smart students\")) \n",
    "print(detect(\"russian - это компьютерный портал для гиков\")) \n",
    "print(detect(\"espanol es un portal informático para computer Applications\")) \n",
    "print(detect(\"norway是面向极客的计算机科学门户\")) \n",
    "print(detect(\"What language is this के लिए एक कंप्यूटर विज्ञान पोर्टल है\")) \n",
    "print(detect(\"I am loving it and は、ギーク向けのコンピューターサイエンスポータルです。\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-6a4a55e972c5>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-6a4a55e972c5>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    N-grams are contiguous sequence of n elements (or words) in a given document.\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# What are n-grams\n",
    "\n",
    "N-grams are contiguous sequence of n elements (or words) in a given document. \n",
    "An item can be a character, a word or a sentence and N can be any integer. \n",
    "When N is 2, we call the sequence a bigram. Similarly, a sequence of 3 items is called a trigram, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why might you use n-grams\n",
    "We could use n-gram models in statistical natural language processing. \n",
    "In speech recognition, phonemes and sequences of phonemes are modeled using a n-gram distribution.\n",
    "Also for auto completion of sentences (such as the one we see in Gmail these days), \n",
    "auto spell check (yes, we can do that as well), and to a certain extent, we can check for grammar in a given sentence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why would there be a risk for overfitting the data with n-grams\n",
    "This is because, either the limits of training data, which can have a limited size or include plenty of\n",
    "noises, or the constraints of algorithms, which are too complicated, and require too many parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is the max_features parameter used for CountVectorizer\n",
    "\n",
    "It is used for CountVectorizer because it is used to ignore words that are too common with MAX_DF. MAX_DF looks at how many documents contained a term, \n",
    "and if it exceeds the MAX_DF threshold, then it is eliminated from consideration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
